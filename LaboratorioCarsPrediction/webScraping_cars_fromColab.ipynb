{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk8/YMoGlemNjL6GhI6QmR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicolasSep/Laboratorio_1/blob/main/webScraping_cars_fromColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Base De Carros\n",
        "\n",
        "Realizado por:\n",
        "\n",
        "\n",
        "*   Nicolas Sepulveda Criollo - 69614\n",
        "*   Julian Rangel - 89721\n",
        "\n"
      ],
      "metadata": {
        "id": "NYKKXUjfcKdE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuldmQ6rcIRJ"
      },
      "outputs": [],
      "source": [
        "!pip install lxml\n",
        "!pip install scrapy\n",
        "!pip3 install requests-html\n",
        "!pip3 install selenium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Install chromedriver\n",
        "# Credits: https://medium.com/@MinatoNamikaze02/running-selenium-on-google-colab-a118d10ca5f8\n",
        "sudo apt -y update\n",
        "sudo apt install -y wget curl unzip\n",
        "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "dpkg -i google-chrome-stable_current_amd64.deb\n",
        "\n",
        "wget -N https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/120.0.6099.62/linux64/chromedriver-linux64.zip -P /tmp/\n",
        "unzip -o /tmp/chromedriver-linux64.zip -d /tmp/\n",
        "chmod +x /tmp/chromedriver-linux64/chromedriver\n",
        "mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver\n",
        "\n",
        "pip install selenium chromedriver_autoinstaller"
      ],
      "metadata": {
        "id": "7r_jF_MqcvCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install undetected_chromedriver"
      ],
      "metadata": {
        "id": "udoHbbODcyNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "credits:\n",
        "https://github.com/googlecolab/colabtools/issues/3347\n",
        "https://stackoverflow.com/questions/51046454/how-can-we-use-selenium-webdriver-in-colab-research-google-com\n",
        "Sept 19, 2023\n",
        "'''\n",
        "\n",
        "#\n",
        "!pip3 install chromedriver-autoinstaller"
      ],
      "metadata": {
        "id": "v0Y1D4Anc8fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from selenium import webdriver\n",
        "import chromedriver_autoinstaller\n",
        "import json"
      ],
      "metadata": {
        "id": "kXaxHGNUdKAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless') # ensure GUI is off\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# # set path to chromedriver as per your configuration\n",
        "chromedriver_autoinstaller.install()"
      ],
      "metadata": {
        "id": "LW_c94Q-dMYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrapebyPages(brand,model,min, max):\n",
        "  #Range of pages from the total search to scrape in.\n",
        "  #It is recomended to cover a range of one hundred pages in each iteration of this section.\n",
        "  data = pd.DataFrame()\n",
        "  for i in range(min,max):\n",
        "\n",
        "      print(f'************************************')\n",
        "      print(f'WEB SCRAPING FROM SEARCH PAGE #{i}')\n",
        "      pag = i\n",
        "      url = f'https://vehiculos.tucarro.com.co/{brand}/{model}/_Desde_{49*i}_NoIndex_True'\n",
        "\n",
        "      driver = webdriver.Chrome(options=chrome_options)\n",
        "      driver.get(url)\n",
        "      driver.implicitly_wait(10)\n",
        "      html = driver.page_source\n",
        "      soup = bs(html,'lxml')\n",
        "\n",
        "      #Get href\n",
        "      links = gethref(soup)\n",
        "\n",
        "      p = []\n",
        "      #Scraping\n",
        "      for i in range(0,len(links)):\n",
        "          print('Scrapping', i, '/', len(links), '...')\n",
        "          p.append(scrapper(links[i]))\n",
        "          print(f'Este es el valor de p[i]: {p[i]}')\n",
        "\n",
        "      # append list to DataFrame\n",
        "      temp_df = pd.DataFrame(p)\n",
        "      data = pd.concat([data, temp_df], ignore_index=True)\n",
        "\n",
        "  #Close the web browser tab\n",
        "  driver.close()\n",
        "\n",
        "  # quit the driver\n",
        "  driver.quit()\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "ibsmPMBXdPl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to get 'href' from each article item\n",
        "def gethref(soup):\n",
        "\n",
        "    links = []\n",
        "    for link in soup.findAll('a'):\n",
        "      url_car = link.get('href')\n",
        "      if 'MCO-' in url_car:\n",
        "        # print(url_car)          %Print each car url as a validity test\n",
        "        links.append(url_car)\n",
        "\n",
        "    print(\"Href obtained: \", len(links))\n",
        "\n",
        "    return links\n",
        "    # return"
      ],
      "metadata": {
        "id": "KvxiMKrZdSOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to call housing_features routine on each href\n",
        "def scrapper(url_car):\n",
        "\n",
        "    # set up the webdriver\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    # Scrape\n",
        "    driver.get(url_car)\n",
        "    driver.implicitly_wait(10)\n",
        "    html=driver.page_source\n",
        "\n",
        "    #Obtaining the html from the web page after applying Selenium\n",
        "    soup = bs(html,'lxml')\n",
        "\n",
        "    #Create a list to store info obtained from one particular property\n",
        "    features = []\n",
        "\n",
        "    #Applying function to obtain variables defined from one particular property\n",
        "    features = extract_cars_features(soup)\n",
        "\n",
        "    #Close the web browser tab\n",
        "    driver.close()\n",
        "\n",
        "    # quit the driver\n",
        "    driver.quit()\n",
        "\n",
        "    return(features)"
      ],
      "metadata": {
        "id": "ypQzJvF3dVAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 1.0\n",
        "def extract_cars_features(soup):\n",
        "\n",
        "  features_list = []\n",
        "\n",
        "  # car_name\n",
        "  try:\n",
        "    car_name = soup.find('h1',{'class': 'ui-pdp-title'}).text\n",
        "    features_list.append(car_name)\n",
        "    # print(f\"Car's name is: {car_name}\")\n",
        "  except:\n",
        "    car_name = ' '\n",
        "    features_list.append(car_name)\n",
        "\n",
        "  # price\n",
        "  try:\n",
        "    price=soup.find('div',{'class': 'ui-pdp-price__second-line'}).text\n",
        "    features_list.append(price)\n",
        "    # print(f\"Car's price is: {price}\")\n",
        "  except:\n",
        "    price = 0\n",
        "    features_list.append(price)\n",
        "\n",
        "  # year_car\n",
        "  try:\n",
        "    year_kms_datePub = soup.find('div',{'class': 'ui-pdp-header__subtitle'}).text.split(' ')\n",
        "    year = year_kms_datePub[0]\n",
        "    features_list.append(year)\n",
        "  except:\n",
        "    year = 0\n",
        "    features_list.append(year)\n",
        "\n",
        "  # kms\n",
        "  try:\n",
        "    year_kms_datePub = soup.find('div',{'class': 'ui-pdp-header__subtitle'}).text.split(' ')\n",
        "    kms = year_kms_datePub[2]\n",
        "    features_list.append(kms)\n",
        "  except:\n",
        "    kms = 0\n",
        "    features_list.append(kms)\n",
        "  # print(f\"Kms: {kms}\")\n",
        "\n",
        "# color and Fuel Type\n",
        "  try:\n",
        "    script = soup.find(\"script\", {'type': 'application/ld+json'})\n",
        "    if script:\n",
        "      # Obtain script content\n",
        "      script_text = json.loads(script.string)\n",
        "\n",
        "      # Extract json keys for color and fuel type\n",
        "      color = script_text.get('color', 'Color not found')\n",
        "      fuel = script_text.get('fuelType','Fuel type not found')\n",
        "\n",
        "      # Append results\n",
        "      features_list.extend([color, fuel])\n",
        "    else:\n",
        "      print(\"JavaScript script was not found on the page.\")\n",
        "  except json.JSONDecodeError as e:\n",
        "      print(\"Error decoding JSON:\", str(e))\n",
        "      # Append default values in case of JSON decoding error\n",
        "      features_list.extend([0, 0])\n",
        "  except Exception as e:\n",
        "      print(\"An unexpected error occurred:\", str(e))\n",
        "      # Handle unexpected errors gracefully\n",
        "      features_list.extend([0, 0])\n",
        "\n",
        "\n",
        "  # print(features_list)\n",
        "\n",
        "\n",
        "  return features_list"
      ],
      "metadata": {
        "id": "ARkuXiMMdYZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By"
      ],
      "metadata": {
        "id": "4OU5T2Todb_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        " The input parameters for the 'scrapebyPages' function are: Brand name, Car model\n",
        " name. Be careful to write the brand and model names exactly as they are in tucarro.com.\n",
        " The third input parameter is the initial results page (always initialize to 1)\n",
        " and the fourth input parameter is the final results page you want to download data from;\n",
        " this parameter depends on the amount of results pages your car returns\n",
        " for the brand and model you want to get data from. So, it is recommended to search\n",
        " the web portal first to find out how many pages of results you can get\n",
        " for the car you want to get data from.\n",
        "\"\"\"\n",
        "\n",
        "car_brand = 'toyota'   # Brand car name. Ej: chevrolet, renault, kia.\n",
        "car_model = 'prado'        # Model car name. Ej: duster, onix, rio.\n",
        "data = scrapebyPages(car_brand,car_model,1,30)\n",
        "# scrapebyPages(1,2)"
      ],
      "metadata": {
        "id": "xGZC4Z0_dd6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['car_model','price','year_model','kms','color','fueltype']\n",
        "data.columns = cols\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "B4V9FEtbdg6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_name=f'usedCarsCol_{car_model}_200624.csv'\n",
        "data.to_csv(saved_name, encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "4imnWaGDdjcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#*****************************\n",
        "#Code for testing in one page\n",
        "#*****************************\n",
        "import json\n",
        "\n",
        "brand = 'kia'   # Brand car name. Ej: chevrolet, renault, kia.\n",
        "model = 'rio'   # Model car name. Ej: duster, onix, rio.\n",
        "\n",
        "# url = f'https://vehiculos.tucarro.com.co/{model}-{brand}'\n",
        "url = f'https://vehiculos.tucarro.com.co/{brand}/{model}/_Desde_{49*1}_NoIndex_True'\n",
        "print(url)\n",
        "\n",
        "#Function to call cars_features routine on each href\n",
        "def scrapper(url_car):\n",
        "\n",
        "    # set up the webdriver\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    # Scrape\n",
        "    driver.get(url_car)\n",
        "    driver.implicitly_wait(10)\n",
        "    html=driver.page_source\n",
        "\n",
        "    #Obtaining the html from the web page after applying Selenium\n",
        "    soup = bs(html,'lxml')\n",
        "\n",
        "    #Create a list to store info obtained from one particular property\n",
        "    features = []\n",
        "\n",
        "    #Applying function to obtain variables defined from one particular property\n",
        "    features = extract_cars_features(soup)\n",
        "\n",
        "    #Close the web browser tab\n",
        "    driver.close()\n",
        "\n",
        "    # quit the driver\n",
        "    driver.quit()\n",
        "\n",
        "    return(features)\n",
        "\n",
        "\n",
        "def extract_cars_features(soup):\n",
        "\n",
        "  features_list = []\n",
        "\n",
        "  # car_name\n",
        "  try:\n",
        "    car_name = soup.find('h1',{'class': 'ui-pdp-title'}).text\n",
        "    features_list.append(car_name)\n",
        "    # print(f\"Car's name is: {car_name}\")\n",
        "  except:\n",
        "    car_name = ' '\n",
        "    features_list.append(car_name)\n",
        "\n",
        "  # price\n",
        "  try:\n",
        "    price=soup.find('div',{'class': 'ui-pdp-price__second-line'}).text\n",
        "    features_list.append(price)\n",
        "    # print(f\"Car's price is: {price}\")\n",
        "  except:\n",
        "    price = 0\n",
        "    features_list.append(price)\n",
        "\n",
        "  # year_car\n",
        "  try:\n",
        "    year_kms_datePub = soup.find('div',{'class': 'ui-pdp-header__subtitle'}).text.split(' ')\n",
        "    year = year_kms_datePub[0]\n",
        "    features_list.append(year)\n",
        "  except:\n",
        "    year = 0\n",
        "    features_list.append(year)\n",
        "\n",
        "  # kms\n",
        "  try:\n",
        "    year_kms_datePub = soup.find('div',{'class': 'ui-pdp-header__subtitle'}).text.split(' ')\n",
        "    kms = year_kms_datePub[2]\n",
        "    features_list.append(kms)\n",
        "  except:\n",
        "    kms = 0\n",
        "    features_list.append(kms)\n",
        "  # print(f\"Kms: {kms}\")\n",
        "\n",
        " # color and Fuel Type\n",
        "  try:\n",
        "    script = soup.find(\"script\", {'type': 'application/ld+json'})\n",
        "    if script:\n",
        "      # Obtain script content\n",
        "      script_text = json.loads(script.string)\n",
        "\n",
        "      # Extract json keys for color and fuel type\n",
        "      color = script_text.get('color', 'Color not found')\n",
        "      fuel = script_text.get('fuelType','Fuel type not found')\n",
        "\n",
        "      # Append results\n",
        "      features_list.extend([color, fuel])\n",
        "    else:\n",
        "      print(\"JavaScript script was not found on the page.\")\n",
        "  except json.JSONDecodeError as e:\n",
        "      print(\"Error decoding JSON:\", str(e))\n",
        "      # Append default values in case of JSON decoding error\n",
        "      features_list.extend([0, 0])\n",
        "  except Exception as e:\n",
        "      print(\"An unexpected error occurred:\", str(e))\n",
        "      # Handle unexpected errors gracefully\n",
        "      features_list.extend([0, 0])\n",
        "\n",
        "  return features_list\n",
        "\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "driver.get(url)\n",
        "driver.implicitly_wait(10)\n",
        "html = driver.page_source\n",
        "soup = bs(html,'lxml')\n",
        "\n",
        "#Get href\n",
        "links = []\n",
        "for link in soup.findAll('a'):\n",
        "  url_car = link.get('href')\n",
        "  if 'MCO-' in url_car:\n",
        "    links.append(url_car)\n",
        "print(\"Href obtained: \", len(links))\n",
        "\n",
        "p = []\n",
        "#Scraping\n",
        "for i in range(0,len(links)):\n",
        "  print('Scrapping', i, '/', len(links), '...')\n",
        "  p.append(scrapper(links[i]))\n",
        "  print(f'Este es el valor de p[i]: {p[i]}')\n",
        "\n",
        "temp_df = pd.DataFrame(p)\n",
        "# data = pd.concat([data, temp_df], ignore_index=True)\n",
        "\n",
        "#Close the web browser tab\n",
        "driver.close()\n",
        "\n",
        "# quit the driver\n",
        "driver.quit()\n",
        "temp_df.head()"
      ],
      "metadata": {
        "id": "V0wTnSyXdpNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referencias\n",
        "---\n",
        "https://github.com/kiteco/kite-python-blog-post-code/blob/master/Web%20Scraping%20Tutorial/script.py\n",
        "\n",
        "https://medium.com/geekculture/scrappy-guide-to-web-scraping-with-python-475385364381\n",
        "\n",
        "https://stackoverflow.com/questions/47730671/python-3-using-requests-does-not-get-the-full-content-of-a-web-page\n",
        "\n",
        "## Colaboracion\n",
        "---\n",
        "Ing. Elias Buitrago"
      ],
      "metadata": {
        "id": "8MCi3h-kdtjB"
      }
    }
  ]
}